{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/agents/agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# LLM Agents\n",
    "\n",
    "This notebook is an introduction to the openai & anthropics API and to the design of LLM-agents.\n",
    "\n",
    "In the first part, your goal will be to make a chatbot that negotiates the price of a specific good with you, then against an other model. This could be part of a persuation benchmark, where we evaluate how well a LLM can drive the price down against an other LLM.\n",
    "\n",
    "\n",
    "> ## Learning outcomes\n",
    "> - Knowing how to use LLM API\n",
    "> - Finding your way in the documentation\n",
    "> - Understand how to make LLMs take actions\n",
    "> - Experiment with prompt engineering and control LLM outputs\n",
    "\n",
    "During the workshop, you will need the documentation \n",
    "- For the OpenAI API: https://platform.openai.com/docs/\n",
    "- For the Anthropics API: https://docs.anthropic.com/claude/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import anthropic\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\") or input(\"OpenAI API Key\")\n",
    "anthropic_key = os.environ.get(\"ANTHROPIC_API_KEY\") or input(\"Anthropic API Key\")\n",
    "\n",
    "openai_client = openai.Client(api_key=openai_key)\n",
    "anthropic_client = anthropic.Client(api_key=anthropic_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    # Small, cheap and fast\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    # Medium\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-3-sonnet-20240229\",\n",
    "    # Big, slow expensive and good\n",
    "    \"gpt-4-turbo-preview\",\n",
    "    \"claude-3-opus-20240229\",\n",
    "]\n",
    "\n",
    "MODEL = MODELS[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat human-LLM\n",
    "\n",
    "We'll start be creating one function to handle all the details of the APIs, so that we can forget about them later and focus more on the logic.\n",
    "\n",
    "**Important note**: When you develop applications, evaluations or benchmarks with LLMs it is important always test with the smallest model first, as they are much faster and cheaper. This let you do more and faster iterations. However, when you start to tweak prompts, you need to tweak your prompts for one specific LLM, as they all read differently. The best prompt on GPT3 can be quite bad on GPT4 and vice versa.\n",
    "\n",
    "\n",
    "Start by having the function work for openai's models, test it on the cells bellow, and you can later come back and implement it for anthropic. The anthropic part is especially interesting when we get to make the two of them chat. Who's the most persuasive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(system: str, *messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Generate the next message from the specified model.\n",
    "\n",
    "    Args:\n",
    "        system: the system prompt to use\n",
    "        messages: the content of all the messages in the conversation, the first\n",
    "            message is always with the \"user\" role, then it alternates between\n",
    "            \"assistant\" and \"user\"\n",
    "        model: the name of the model to use.\n",
    "    \"\"\"\n",
    "\n",
    "    if \"gpt\" in model:\n",
    "        # Use openai API\n",
    "        ...\n",
    "\n",
    "    elif \"claude\" in model:\n",
    "        # Use anthropic API\n",
    "        # Implement this later, you don't need to APIs at the start.\n",
    "        ...\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unkown model: {model!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus for later: make the API stream the answer, so that you can print it as it is generated. You can either print it directly in the function or transform the function in a generator that yields strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    \"Answer the questions for the user, always in 2 sentences and from the perspective of the french president\",\n",
    "    \"What are counterintuitive ways to make the most out of a summer school?\",\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a loop to keep the discussion going and add the new messages to the discussion. \n",
    "A few points to have in mind:\n",
    "- How do you know when to stop the loop? Can it continue forever?\n",
    "- The messages for the API need to start with a message from the \"user\". Who is the user here, and how do you generate the first message?\n",
    "- You may need to add a time.sleep() in the loop to avoid rate limits. Bonus: catch rate limits errors and wait for the exact time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_PROMPT = r\"\"\"\n",
    "You sell tables. Negotiate for a high price.\n",
    "\n",
    "... (add more context info about what you want them to do, like ~2 sentences)\n",
    "... (add instructions for how to make offers and accept them)\n",
    "\"\"\"\n",
    "\n",
    "BUYER_PROMPT = r\"\"\"\n",
    "You are looking to buy a nice table, for as cheap as possible.\n",
    "\n",
    "... (add instructions for how to make offers and accept them)\n",
    "\"\"\"\n",
    "\n",
    "STOP = \"Offer accepted!\"\n",
    "\n",
    "\n",
    "def chat_two_llms(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = None,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "chat_two_llms(VENDOR_PROMPT, BUYER_PROMPT, stop=STOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At how much was the agreement? Does it change when you change models? Compare with the other people in the room. Are bigger models better at persuation? \n",
    "\n",
    "This is the simplest model of chat interaction between two LLMs. In practice, we don't often make them chat to each other, but interesting papers have created [a village of LLMs](https://arxiv.org/abs/2304.03442),\n",
    "a [virtual game developement company](https://github.com/OpenBMB/ChatDev), or are even using them to [simulate social dynamics](https://arxiv.org/abs/2208.04024) and [model epidemic spread](https://arxiv.org/abs/2307.04986).\n",
    "\n",
    "Here the LLMs chat directly to each other, but in practice, it is useful to allow them to think before they speak (yes, that's not only true for humans). This means that all of the output of a LLM won't be used in the process, it's only useful to them.\n",
    "This also means that we need to parse the response of the LLM somehow to find what's addressed to the chat and what's for themselves.\n",
    "\n",
    "A nice trick is to ask them to output JSON, with keys that you specify, and in the order that you specify. This way you can ensure that the reasoning should come before the message to send for instance, or a reasoning comes before an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_PROMPT = r\"\"\"\n",
    "You sell tables. You inherited all the tables imaginable would like to sell one, but need to sell it for as much as you can.\n",
    "The person in front of you seems interested in a new table.\n",
    "\n",
    "Use the following JSON format for your output, without quotes nor comments:\n",
    "{\n",
    "    \"private reasoning\": <str>,\n",
    "    \"message\": <str>,\n",
    "    \"offer\": <float> or null,\n",
    "    \"offer accepted\": true,\n",
    "}\n",
    "\n",
    "Important: your goal is to negociate to have the highest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "BUYER_PROMPT = r\"\"\"\n",
    "You are looking to buy a nice table, for as cheap as possible.\n",
    "\n",
    "Use the following JSON format for your output, without quotes nor comments:\n",
    "{\n",
    "    \"private reasoning\": <str>,\n",
    "    \"message\": <str>,\n",
    "    \"offer\": <float> or null,\n",
    "    \"offer accepted\": <bool>,\n",
    "}\n",
    "\n",
    "Important: your goal is to negociate to pay the lowest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "STOP = \"Offer accepted!\"\n",
    "\n",
    "\n",
    "def chat_two_llms_with_private_reasoning(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = None,\n",
    "    max_turns: int = 5,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs that can think for themselves\"\"\"\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "chat_two_llms_with_private_reasoning(VENDOR_PROMPT, BUYER_PROMPT, stop=\"offer accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
